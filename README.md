# ImageCaptioning_Using_DeepNeuralNetworks
Captioning images automatically is one of the heart of the human visual system. There are
various advantages if there is an application which automatically caption the scenes
surrounded by them and revert back the caption as a plain message. In this project, we present
a model based on CNN-LSTM neural networks which automatically detects the objects in the
images and generates descriptions for the images. It uses various pre-trained models to
perform the task of detecting objects and uses CNN and LSTM to generate the captions. It
uses Transfer Learning based pre-trained models for the task of object Detection. This model
can perform two operations. The first one is to detect objects in the image using
Convolutional Neural Networks and the other is to caption the images using RNN based
LSTM(Long Short Term Memory). Caption generation is one of the interesting and focussed areas of Artificial
Intelligence which has many challenges to pass on. Caption generation involves various
complex scenarios starting from picking the dataset, training the model, validating the model,
creating pre-trained models to test the images ,detecting the images and finally generating the
captions. The model is trained in such a way that if input image is given
to model it generates captions which nearly describes the image.
The accuracy of model and smoothness or command of language
model learns from image descriptions is tested on different
datasets. These experiments show that model is frequently giving
accurate descriptions for an input image.

## 1.Download Flickr8k dataset through the below link:

             https://illinois.edu/fb/sec/1713398
